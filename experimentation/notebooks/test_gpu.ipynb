{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the ImageNet dataset for training\n",
    "! python project_module/src/project_module/theia/scripts/preprocessing/image_datasets/organize_imagenet_webdataset.py --dataset imagenet-mini --imagenet-raw-path experimentation/dataset/temp/imagenet-mini --output-path experimentation/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the ImageNet dataset\n",
    "! python -m project_module.theia.scripts.preprocessing.feature_extraction --dataset imagenet-mini --dataset-root /home/tomo0530/theia-demo/experimentation/dataset --output-path /home/tomo0530/theia-demo/experimentation/dataset/imagenet-mini/feature --model facebook/dinov2-with-registers-base --split train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5caebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "from torchvision.io import read_video, write_video\n",
    "from project_module.theia.decoding import load_feature_stats, prepare_depth_decoder, prepare_mask_generator, decode_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd31d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-tiny-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "theia_model = AutoModel.from_pretrained(\"theaiinstitute/theia-tiny-patch16-224-cddsv\", trust_remote_code=True)\n",
    "theia_model = theia_model.to(device)\n",
    "target_model_names = [\n",
    "    \"google/vit-huge-patch14-224-in21k\",\n",
    "    \"facebook/dinov2-large\",\n",
    "    \"openai/clip-vit-large-patch14\",\n",
    "    \"facebook/sam-vit-huge\",\n",
    "    \"LiheYoung/depth-anything-large-hf\",\n",
    "]\n",
    "feature_means, feature_vars = load_feature_stats(target_model_names, stat_file_root=\"/home/tomo0530/theia-demo/experimentation/dataset/feature_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the mask generator and depth decoder\n",
    "mask_generator, sam_model = prepare_mask_generator(device)\n",
    "depth_anything_model_name = \"LiheYoung/depth-anything-large-hf\"\n",
    "depth_anything_decoder, _ = prepare_depth_decoder(depth_anything_model_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b116a6",
   "metadata": {},
   "source": [
    "### Example Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae9698b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomo0530/theia-demo/.venv/lib/python3.10/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "example_video_path = \"/home/tomo0530/theia-demo/experimentation/dataset/video/example_video_to_visualize.mp4\"\n",
    "video, _, _ = read_video(example_video_path, pts_unit=\"sec\", output_format=\"THWC\")\n",
    "video = video.numpy()\n",
    "images = [Image.fromarray(cv2.resize(im, (224, 224))) for im in video]\n",
    "\n",
    "theia_decode_results, gt_decode_results = decode_everything(\n",
    "    theia_model=theia_model,\n",
    "    feature_means=feature_means,\n",
    "    feature_vars=feature_vars,\n",
    "    images=images,\n",
    "    mask_generator=mask_generator,\n",
    "    sam_model=sam_model,\n",
    "    depth_anything_decoder=depth_anything_decoder,\n",
    "    pred_iou_thresh=0.5,\n",
    "    stability_score_thresh=0.7,\n",
    "    gt=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c6b6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomo0530/theia-demo/.venv/lib/python3.10/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vis_video = np.stack(\n",
    "    [np.vstack([tr, gtr]) for tr, gtr in zip(theia_decode_results, gt_decode_results, strict=False)]\n",
    ")\n",
    "vis_video = torch.from_numpy(vis_video * 255.0).to(torch.uint8)\n",
    "vis_save_path = \"/home/tomo0530/theia-demo/experimentation/outputs/visualized.mp4\"\n",
    "write_video(vis_save_path, vis_video, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15eaec",
   "metadata": {},
   "source": [
    "### Pizza Pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f5a5ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomo0530/theia-demo/.venv/lib/python3.10/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pizza_video_path = \"/home/tomo0530/theia-demo/experimentation/dataset/video/pizza_pick.mp4\"\n",
    "video, _, _ = read_video(pizza_video_path, pts_unit=\"sec\", output_format=\"THWC\")\n",
    "video = video.numpy()\n",
    "images = [Image.fromarray(cv2.resize(im, (224, 224))) for im in video]\n",
    "\n",
    "theia_decode_results, gt_decode_results = decode_everything(\n",
    "    theia_model=theia_model,\n",
    "    feature_means=feature_means,\n",
    "    feature_vars=feature_vars,\n",
    "    images=images,\n",
    "    mask_generator=mask_generator,\n",
    "    sam_model=sam_model,\n",
    "    depth_anything_decoder=depth_anything_decoder,\n",
    "    pred_iou_thresh=0.5,\n",
    "    stability_score_thresh=0.7,\n",
    "    gt=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "vis_video = np.stack(\n",
    "    [np.vstack([tr, gtr]) for tr, gtr in zip(theia_decode_results, gt_decode_results, strict=False)]\n",
    ")\n",
    "vis_video = torch.from_numpy(vis_video * 255.0).to(torch.uint8)\n",
    "vis_save_path = \"/home/tomo0530/theia-demo/experimentation/outputs/visualized_pizza_pick.mp4\"\n",
    "write_video(vis_save_path, vis_video, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ac1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    \"/home/tomo0530/theia-demo/experimentation/outputs/theia_decode_results.npy\",\n",
    "    theia_decode_results,\n",
    ")\n",
    "np.save(\n",
    "    \"/home/tomo0530/theia-demo/experimentation/outputs/gt_decode_results.npy\",\n",
    "    gt_decode_results,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
